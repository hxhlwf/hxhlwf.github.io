---
layout: post
category: "rl"
title: "dual learning for mt"
tags: [dual learning, mt]
---


目录

<!-- TOC -->


<!-- /TOC -->
[dual-learning-for-machine-translation.pdf](../assets/rl/dual-learning-nmt/dual-learning-for-machine-translation.pdf)

深度学习之所以能够取得巨大的成功，一个非常重要的因素就是大数据，特别是大规模的带标签的数据。例如在图像识别中，深度神经网络使用上百万的带标签的图像进行训练，在机器翻译中我们会用上千万的双语句对进行训练，在围棋中我们会用上千万的专业棋手的落子进行训练……这种做法有两个局限性。首先，人工标注获取标签的代价很高。例如我们考虑机器翻译这个任务：现在市场人工翻译一个单词的价格差不多是5到10美分，如果一个句子的平均长度为三十个单词，那么1000万个句子人工翻译的代价差不多是7.5美分×30×1000万，约等于2200万美元。现在一个商业公司的翻译引擎通常支持上百种语言的相互翻译，为了训练这样规模的翻译模型，人工标注的代价就会达到上千亿美元。其次，在很多任务中，我们没办法收集到大规模的标注数据，例如在医疗中或在小语种的相互翻译。为了使深度学习能够取得更广泛的成功，我们需要降低其对大规模标注数据的依赖性。为了解决这个问题，我们提出了一种新的学习范式，我们把它称作对偶学习。 

考虑一个对偶翻译游戏，里面有两个玩家小明和爱丽丝，如下图所示。小明只能讲中文，爱丽丝只会讲英文，他们两个人一起希望能够提高英文到中文的翻译模型f和中文到英文的翻译模型g。给定一个英文的句子x，爱丽丝首先通过f把这个句子翻译成中文句子y1，然后把这个中文的句子发给小明。因为没有标注，所以小明不知道正确的翻译是什么，但是小明可以知道，这个中文的句子是不是语法正确、符不符合中文的语言模型，这些信息都能帮助小明大概判断翻译模型f是不是做的好。然后小明再把这个中文的句子y1通过翻译模型g翻译成一个新的英文句子x1，并发给爱丽丝。通过比较x和x1是不是相似，爱丽丝就能够知道翻译模型f和g是不是做得好，尽管x只是一个没有标注的句子。因此，通过这样一个对偶游戏的过程，我们能够从没有标注的数据上获得反馈，从而知道如何提高机器学习模型。

实际上这个对偶游戏和强化学习的过程比较类似。在强化学习中，我们希望提高我们的策略以最大化长远的回报，但是没有标注的样本告诉我们在某个状态x哪个动作y是正确的。我们只有通过使用这个策略在不同的状态下执行不同的动作，观测该动作带来的回报，从而改善我们的策略。在以上这个翻译对偶游戏中，两个翻译模型就是我们的策略，因为没有标注的双语句对，所以我们不能直接改善它们。这个对偶游戏把一个没有标注的句子x，先翻译成另外一种语言的句子y1，再翻译回来为x1，这里x就是强化学习中的状态，y1和x1就是我们的策略所执行的动作，x和x1的相似度就是我们获得的回报。

我们可以用已有的强化学习的算法来训练我们这两个翻译模型，比如策略梯度方法。策略梯度方法的基本思想非常简单：如果我们在执行某个动作之后，观测到了一个很大的回报，我们就通过调整策略（在当前策略函数的参数上加上它的梯度）来增加这个状态下执行这个动作的概率；相反，如果我们在执行某个动作之后，观测到了一个很小的回报，甚至是负的回报，那么我们就需要调整策略（在当前策略函数的参数上减去它的梯度），以降低在这个状态下执行这个动作的概率。
